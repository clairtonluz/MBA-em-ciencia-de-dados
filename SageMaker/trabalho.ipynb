{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de Modelo de Machine Learning usando AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membros\n",
    "* 2315530 - Clairton Carneiro Luz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este documento apresenta um guia para o treinamento de um modelo de Machine Learning utilizando o AWS SageMaker. O objetivo desta atividade é permitir a prática na implementação de um pipeline de aprendizado de máquina, desde a preparação do ambiente até a implantação do modelo treinado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desta atividade é desenvolver habilidades na utilização do AWS SageMaker para treinar e implantar modelos de Machine Learning. O processo envolve a criação de um ambiente adequado, manipulação de dados, treinamento do modelo, avaliação de desempenho e implantação para uso em produção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparação do Ambiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 3.1 Criar um notebook instance no SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Acesse o console AWS e vá para o serviço `Amazon SageMaker AI`.\n",
    "\n",
    "* No painel esquerdo, clique em `Notebooks`.\n",
    "* Clique em `Create notebook instance`.\n",
    "* Defina um nome para a instância e escolha um tipo adequado, como ml.t3.medium.\n",
    "* Em IAM role, selecione Create a new role, permitindo acesso ao Amazon S3.\n",
    "* Clique em Create notebook instance e aguarde até que o status mude para InService.\n",
    "\n",
    "* Agora faça o upload desse notebook para que possa executa-lo no ambiente em nuvem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Configurar as permissões IAM para permitir o acesso ao S3 e SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No console AWS, vá para o serviço IAM.\n",
    "\n",
    "* Clique em Roles e selecione a função atribuída ao notebook SageMaker.\n",
    "* Em Permissões, adicione as políticas AmazonS3FullAccess e AmazonSageMakerFullAccess.\n",
    "* Salve as alterações para garantir que o SageMaker possa acessar os recursos necessários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Instalar as bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Após iniciar o notebook, execute o seguinte comando para instalar as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U sagemaker pandas numpy scikit-learn kagglehub matplotlib seaborn\n",
    "%pip install -q boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download e Preparação dos Dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/eslamelsolya/laptop-price-prediction?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 398k/398k [00:00<00:00, 22.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /home/ec2-user/.cache/kagglehub/datasets/eslamelsolya/laptop-price-prediction/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 2.3GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 640</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>71378.6832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1440x900</td>\n",
       "      <td>Intel Core i5 1.8GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics 6000</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>47895.5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Full HD 1920x1080</td>\n",
       "      <td>Intel Core i5 7200U 2.5GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel HD Graphics 620</td>\n",
       "      <td>No OS</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>30636.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>15.4</td>\n",
       "      <td>IPS Panel Retina Display 2880x1800</td>\n",
       "      <td>Intel Core i7 2.7GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>AMD Radeon Pro 455</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>135195.3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 3.1GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 650</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>96095.8080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company   TypeName  Inches                    ScreenResolution  \\\n",
       "0   Apple  Ultrabook    13.3  IPS Panel Retina Display 2560x1600   \n",
       "1   Apple  Ultrabook    13.3                            1440x900   \n",
       "2      HP   Notebook    15.6                   Full HD 1920x1080   \n",
       "3   Apple  Ultrabook    15.4  IPS Panel Retina Display 2880x1800   \n",
       "4   Apple  Ultrabook    13.3  IPS Panel Retina Display 2560x1600   \n",
       "\n",
       "                          Cpu   Ram               Memory  \\\n",
       "0        Intel Core i5 2.3GHz   8GB            128GB SSD   \n",
       "1        Intel Core i5 1.8GHz   8GB  128GB Flash Storage   \n",
       "2  Intel Core i5 7200U 2.5GHz   8GB            256GB SSD   \n",
       "3        Intel Core i7 2.7GHz  16GB            512GB SSD   \n",
       "4        Intel Core i5 3.1GHz   8GB            256GB SSD   \n",
       "\n",
       "                            Gpu  OpSys  Weight        Price  \n",
       "0  Intel Iris Plus Graphics 640  macOS  1.37kg   71378.6832  \n",
       "1        Intel HD Graphics 6000  macOS  1.34kg   47895.5232  \n",
       "2         Intel HD Graphics 620  No OS  1.86kg   30636.0000  \n",
       "3            AMD Radeon Pro 455  macOS  1.83kg  135195.3360  \n",
       "4  Intel Iris Plus Graphics 650  macOS  1.37kg   96095.8080  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"eslamelsolya/laptop-price-prediction\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "df_original = pd.read_csv(path + \"/laptop_data.csv\", index_col=0)\n",
    "\n",
    "df = df_original.copy()\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Realizar a limpeza e pré-processamento dos dados (tratamento de valores faltantes, normalização, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1303 entries, 0 to 1302\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Company           1303 non-null   object \n",
      " 1   TypeName          1303 non-null   object \n",
      " 2   Inches            1303 non-null   float64\n",
      " 3   ScreenResolution  1303 non-null   object \n",
      " 4   Cpu               1303 non-null   object \n",
      " 5   Ram               1303 non-null   object \n",
      " 6   Memory            1303 non-null   object \n",
      " 7   Gpu               1303 non-null   object \n",
      " 8   OpSys             1303 non-null   object \n",
      " 9   Weight            1303 non-null   object \n",
      " 10  Price             1303 non-null   float64\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 122.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.duplicated().sum())\n",
    "df.drop_duplicates(inplace = True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company    TypeName          \n",
       "Acer       Notebook               76\n",
       "           2 in 1 Convertible      8\n",
       "           Gaming                  8\n",
       "           Netbook                 5\n",
       "           Ultrabook               4\n",
       "Apple      Ultrabook              21\n",
       "Asus       Notebook               62\n",
       "           Gaming                 54\n",
       "           Ultrabook              18\n",
       "           2 in 1 Convertible     13\n",
       "           Netbook                 4\n",
       "Chuwi      Notebook                3\n",
       "Dell       Notebook              159\n",
       "           Ultrabook              49\n",
       "           Gaming                 40\n",
       "           2 in 1 Convertible     30\n",
       "           Workstation            11\n",
       "           Netbook                 2\n",
       "Fujitsu    Notebook                3\n",
       "Google     Ultrabook               3\n",
       "HP         Notebook              180\n",
       "           Ultrabook              36\n",
       "           2 in 1 Convertible     19\n",
       "           Workstation            14\n",
       "           Gaming                 12\n",
       "           Netbook                 7\n",
       "Huawei     Ultrabook               2\n",
       "LG         Ultrabook               3\n",
       "Lenovo     Notebook              174\n",
       "           2 in 1 Convertible     44\n",
       "           Gaming                 32\n",
       "           Ultrabook              31\n",
       "           Netbook                 4\n",
       "           Workstation             4\n",
       "MSI        Gaming                 54\n",
       "Mediacom   Notebook                6\n",
       "           2 in 1 Convertible      1\n",
       "Microsoft  Ultrabook               6\n",
       "Razer      Gaming                  5\n",
       "           Ultrabook               2\n",
       "Samsung    Ultrabook               5\n",
       "           2 in 1 Convertible      2\n",
       "           Netbook                 1\n",
       "           Notebook                1\n",
       "Toshiba    Notebook               36\n",
       "           Ultrabook              12\n",
       "Vero       Notebook                4\n",
       "Xiaomi     Notebook                2\n",
       "           Ultrabook               2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Company\")[\"TypeName\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company             0\n",
       "TypeName            0\n",
       "Inches              0\n",
       "ScreenResolution    0\n",
       "Cpu                 0\n",
       "Ram                 0\n",
       "Memory              0\n",
       "Gpu                 0\n",
       "OpSys               0\n",
       "Weight              0\n",
       "Price               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.37kg' '1.34kg' '1.86kg' ... '1.3kg' '1.5kg' '2.19kg']\n"
     ]
    }
   ],
   "source": [
    "# Removendo unidades de medida e convertendo para float\n",
    "print(df[\"Weight\"].values)\n",
    "df[\"Weight\"] = df[\"Weight\"].str.replace(\"kg\", \"\")\n",
    "df[\"Weight\"] = df[\"Weight\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8GB' '8GB' '8GB' ... '16GB' '2GB' '6GB']\n"
     ]
    }
   ],
   "source": [
    "# removendo unidades de medida e convertendo para float\n",
    "print(df[\"Ram\"].values)\n",
    "df[\"Ram\"] = df[\"Ram\"].str.replace(\"GB\", \"\")\n",
    "df[\"Ram\"] = df[\"Ram\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Convertendo colunas categoricas para numericas\n",
    "label_encoder = LabelEncoder()\n",
    "for col in df.select_dtypes(include = [\"object\"]).columns:\n",
    "  df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(df.corr(), annot = True, fmt = \".2g\", cmap = \"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Dividir o dataset em conjuntos de treino e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Separando features e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = df.drop(\"Price\", axis = 1)\n",
    "y = np.log(df[\"Price\"] + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Separando treino, teste e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Transformando valores para ficar no mesmo padrão de escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Scaling_Features = StandardScaler()\n",
    "X_train = Scaling_Features.fit_transform(X_train)\n",
    "X_test = Scaling_Features.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Armazenamento dos Dados no S3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Criar um bucket S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No console AWS, vá para o serviço S3.\n",
    "\n",
    "* Clique em Create bucket e defina um nome único.\n",
    "\n",
    "* Configure permissões adequadas e finalize a criação.\n",
    "\n",
    "\n",
    "#### Outra opção é utilizar o bucket default que o sagemaker já configura pra você com o comando `sagemaker_session.default_bucket()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Upload dos dados de treino, teste e validação para o bucket S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os dados de treino, teste e validação no s3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "# caso queira utilizar um bucket especifico usa a linha abaixo para definir o bucket no lugar da linha acima.\n",
    "# bucket = \"unifor-aws-sagemake\"\n",
    "prefix = \"laptop-price-prediction\"\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "pd.concat([pd.DataFrame(y_train), pd.DataFrame(X_train)],\n",
    "          axis=1).to_csv(train_file, header=False, index=False)\n",
    "pd.concat([pd.DataFrame(y_test), pd.DataFrame(X_test)],\n",
    "          axis=1).to_csv(test_file, header=False, index=False)\n",
    "\n",
    "sagemaker_session.upload_data(train_file, bucket, prefix + \"/train\")\n",
    "sagemaker_session.upload_data(test_file, bucket, prefix + \"/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento do Modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Escolher um algoritmo de machine learning disponível no SageMaker (por exemplo, XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Defina o papel de execução do SageMaker\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Configurar o job de treinamento no SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criar e treinar modelos\n",
    "models = {\n",
    "    \"xgboost\": Estimator(\n",
    "        image_uri=sagemaker.image_uris.retrieve(\n",
    "            \"xgboost\", sagemaker_session.boto_region_name, version=\"1.5-1\"),\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        output_path=f\"s3://{bucket}/{prefix}/models/xgboost\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        hyperparameters={\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"num_round\": \"100\"\n",
    "        }\n",
    "    )\n",
    "    # \"linear\": Estimator(\n",
    "    #     image_uri=sagemaker.image_uris.retrieve(\n",
    "    #         \"linear-learner\", sagemaker_session.boto_region_name),\n",
    "    #     role=role,\n",
    "    #     instance_count=1,\n",
    "    #     instance_type=\"ml.m5.large\",\n",
    "    #     output_path=f\"s3://{bucket}/{prefix}/models/linear\",\n",
    "    #     sagemaker_session=sagemaker_session,\n",
    "    #     hyperparameters={\n",
    "    #         \"predictor_type\": \"regressor\"\n",
    "    #     }\n",
    "    # ),\n",
    "    # \"random_forest\": Estimator(\n",
    "    #     image_uri=sagemaker.image_uris.retrieve(\n",
    "    #         \"randomcutforest\", sagemaker_session.boto_region_name),\n",
    "    #     role=role,\n",
    "    #     instance_count=1,\n",
    "    #     instance_type=\"ml.m5.large\",\n",
    "    #     output_path=f\"s3://{bucket}/{prefix}/models/random_forest\",\n",
    "    #     sagemaker_session=sagemaker_session,\n",
    "    #     hyperparameters={\n",
    "    #         \"num_samples_per_tree\": \"512\",\n",
    "    #         \"num_trees\": \"50\",\n",
    "    #         \"feature_dim\": str(X_train.shape[1])\n",
    "    #     }\n",
    "    # )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Iniciar o treinamento do modelo utilizando os dados armazenados no S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "\n",
    "train_key = f\"s3://{bucket}/{prefix}/train/{train_file}\"\n",
    "train_input = TrainingInput(\n",
    "    train_key,\n",
    "    content_type=\"text/csv\"\n",
    ")\n",
    "\n",
    "# for model_name, estimator in models.items():\n",
    "#     estimator.fit({\"train\": train_input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"xgboost\"].fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "xgb_predictor = models[\"xgboost\"].deploy(\n",
    "    initial_instance_count=1, \n",
    "    serializer=CSVSerializer(),\n",
    "    instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models[\"linear\"].fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"random_forest\"].fit({\"train\": TrainingInput(\n",
    "    train_key,\n",
    "    content_type=\"text/csv\",\n",
    "    distribution=\"ShardedByS3Key\"\n",
    ")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avaliação do Modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Realizar previsões utilizando o conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação dos modelos\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.predictor import Predictor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "best_model = \"xboost\"\n",
    "best_rmse = float(\"inf\")\n",
    "# rmse_scores = {}\n",
    "\n",
    "# models2 = {\n",
    "#     \"xgboost\": models[\"xgboost\"]\n",
    "# }\n",
    "\n",
    "# for model_name, estimator in models.items():\n",
    "#     print(estimator.latest_training_job.name)\n",
    "#     predictor = Predictor(\n",
    "#         endpoint_name=estimator.latest_training_job.name,\n",
    "#         sagemaker_session=sagemaker_session,\n",
    "#         serializer=CSVSerializer()\n",
    "#     )\n",
    "\n",
    "\n",
    "y_pred = np.array([float(xgb_predictor.predict(x)) for x in X_test])\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"{model_name}: RMSE = {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprima um grafico que mostre a curva de aprendizado do modelo mostrando a linha o rmse do treino e do teste\n",
    "# conforme o numero de amostras aumenta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_learning_curve(model, X_train, y_train, X_test, y_test):\n",
    "    train_rmse = []\n",
    "    test_rmse = []\n",
    "    for i in range(1, len(X_train) + 1):\n",
    "        model.fit(X_train[:i], y_train[:i])\n",
    "        y_train_pred = model.predict(X_train[:i])\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        train_rmse.append(np.sqrt(mean_squared_error(y_train[:i], y_train_pred)))\n",
    "        test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    plt.plot(train_rmse, label=\"train\")\n",
    "    plt.plot(test_rmse, label=\"test\")\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curve(models[\"xgboost\"], X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "y_pred = predict(X_test)\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot(range(0, 20), range(0, 20), color=\"red\")\n",
    "plt.xlabel(\"True Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_client = sagemaker_session.boto_session.client(\"s3\")\n",
    "response = s3_client.get_object(\n",
    "    Bucket=bucket, Key=f\"{prefix}/output/test.csv.out\")\n",
    "response_body = response[\"Body\"].read()\n",
    "\n",
    "predictions = pd.read_csv(io.BytesIO(response_body), header=None)\n",
    "predictions = predictions.squeeze()\n",
    "\n",
    "\n",
    "mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Avaliar o desempenho do modelo utilizando métricas apropriadas (acurácia, precisão, recall, F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Gerar um relatório de avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Testando o modelo\n",
    "y_pred = predictor.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Salvando o modelo\n",
    "model_path = sklearn.model_data\n",
    "model_path\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).download_file(model_path, \"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Implantação do Modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Implantar o modelo treinado como um endpoint no SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy do modelo\n",
    "predictor = sklearn.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = \"ml.t2.medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Testar o endpoint realizando previsões em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validando o modelo\n",
    "y_pred = predictor.predict(X_val)\n",
    "\n",
    "mean_squared_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Removendo endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletando o endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
